Training Alpaca-LoRA model with params:
base_model: /data/haotian/RAP_tune/llama-30B-hf
data_path: ../data/gsm8k_first_1100.json
output_dir: ./lora-gsm8k-1100-July-22
batch_size: 16
micro_batch_size: 1
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 2048
val_set_size: 0
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: False
add_eos_token: False
group_by_length: False
wandb_project: 30B-LLAMA-8bit
wandb_run_name: July-22
wandb_watch: gradients
wandb_log_model: true
resume_from_checkpoint: False
prompt template: gsm8k

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:46,  7.82s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:16<00:40,  8.11s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:21<00:27,  6.83s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:26<00:18,  6.08s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:30<00:11,  5.56s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:36<00:05,  5.43s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:39<00:00,  4.65s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:39<00:00,  5.60s/it]
/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Found cached dataset json (/data/haotian/.cache/huggingface/datasets/json/default-69ed40db2ea53192/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 594.43it/s]
trainable params: 12,779,520 || all params: 32,541,723,136 || trainable%: 0.03927118409369777
Map:   0%|          | 0/1100 [00:00<?, ? examples/s]Map:   1%|▏         | 14/1100 [00:00<00:08, 128.87 examples/s]Map:   3%|▎         | 28/1100 [00:00<00:08, 131.37 examples/s]Map:   4%|▍         | 42/1100 [00:00<00:07, 133.26 examples/s]Map:   5%|▌         | 56/1100 [00:00<00:07, 132.14 examples/s]Map:   6%|▋         | 70/1100 [00:00<00:07, 133.71 examples/s]Map:   8%|▊         | 84/1100 [00:00<00:07, 133.49 examples/s]Map:   9%|▉         | 98/1100 [00:00<00:07, 132.81 examples/s]Map:  10%|█         | 112/1100 [00:00<00:07, 133.09 examples/s]Map:  11%|█▏        | 126/1100 [00:00<00:07, 133.99 examples/s]Map:  13%|█▎        | 140/1100 [00:01<00:07, 134.80 examples/s]Map:  14%|█▍        | 154/1100 [00:01<00:06, 135.27 examples/s]Map:  15%|█▌        | 168/1100 [00:01<00:06, 135.08 examples/s]Map:  17%|█▋        | 182/1100 [00:01<00:06, 134.00 examples/s]Map:  18%|█▊        | 196/1100 [00:01<00:06, 134.11 examples/s]Map:  19%|█▉        | 210/1100 [00:01<00:06, 134.27 examples/s]Map:  20%|██        | 224/1100 [00:01<00:06, 134.58 examples/s]Map:  22%|██▏       | 238/1100 [00:01<00:06, 132.84 examples/s]Map:  23%|██▎       | 252/1100 [00:01<00:06, 132.36 examples/s]Map:  24%|██▍       | 266/1100 [00:01<00:06, 132.94 examples/s]Map:  25%|██▌       | 280/1100 [00:02<00:06, 132.38 examples/s]Map:  27%|██▋       | 294/1100 [00:02<00:06, 133.14 examples/s]Map:  28%|██▊       | 308/1100 [00:02<00:06, 131.88 examples/s]Map:  29%|██▉       | 322/1100 [00:02<00:05, 131.82 examples/s]Map:  31%|███       | 336/1100 [00:02<00:05, 131.30 examples/s]Map:  32%|███▏      | 350/1100 [00:02<00:05, 131.05 examples/s]Map:  33%|███▎      | 364/1100 [00:02<00:05, 131.12 examples/s]Map:  35%|███▍      | 384/1100 [00:02<00:05, 129.55 examples/s]Map:  36%|███▌      | 398/1100 [00:03<00:05, 130.05 examples/s]Map:  37%|███▋      | 412/1100 [00:03<00:05, 130.01 examples/s]Map:  39%|███▊      | 426/1100 [00:03<00:05, 131.05 examples/s]Map:  40%|████      | 440/1100 [00:03<00:05, 130.85 examples/s]Map:  41%|████▏     | 454/1100 [00:03<00:04, 130.81 examples/s]Map:  43%|████▎     | 468/1100 [00:03<00:04, 131.48 examples/s]Map:  44%|████▍     | 482/1100 [00:03<00:04, 131.07 examples/s]Map:  45%|████▌     | 496/1100 [00:03<00:04, 131.00 examples/s]Map:  47%|████▋     | 516/1100 [00:03<00:04, 130.08 examples/s]Map:  48%|████▊     | 530/1100 [00:04<00:04, 131.61 examples/s]Map:  49%|████▉     | 544/1100 [00:04<00:04, 133.24 examples/s]Map:  51%|█████     | 558/1100 [00:04<00:04, 134.22 examples/s]Map:  52%|█████▏    | 572/1100 [00:04<00:03, 135.48 examples/s]Map:  53%|█████▎    | 586/1100 [00:04<00:03, 135.71 examples/s]Map:  55%|█████▍    | 600/1100 [00:04<00:03, 135.79 examples/s]Map:  56%|█████▌    | 614/1100 [00:04<00:03, 135.52 examples/s]Map:  57%|█████▋    | 628/1100 [00:04<00:03, 135.99 examples/s]Map:  58%|█████▊    | 642/1100 [00:04<00:03, 134.69 examples/s]Map:  60%|█████▉    | 656/1100 [00:04<00:03, 133.63 examples/s]Map:  61%|██████    | 670/1100 [00:05<00:03, 134.07 examples/s]Map:  62%|██████▏   | 684/1100 [00:05<00:03, 133.79 examples/s]Map:  63%|██████▎   | 698/1100 [00:05<00:02, 134.45 examples/s]Map:  65%|██████▍   | 712/1100 [00:05<00:02, 134.89 examples/s]Map:  66%|██████▌   | 726/1100 [00:05<00:02, 135.07 examples/s]Map:  67%|██████▋   | 740/1100 [00:05<00:02, 135.14 examples/s]Map:  69%|██████▊   | 754/1100 [00:05<00:02, 134.73 examples/s]Map:  70%|██████▉   | 768/1100 [00:05<00:02, 134.74 examples/s]Map:  71%|███████   | 782/1100 [00:05<00:02, 132.56 examples/s]Map:  72%|███████▏  | 796/1100 [00:05<00:02, 133.14 examples/s]Map:  74%|███████▎  | 810/1100 [00:06<00:02, 133.34 examples/s]Map:  75%|███████▍  | 824/1100 [00:06<00:02, 133.12 examples/s]Map:  76%|███████▌  | 838/1100 [00:06<00:01, 133.49 examples/s]Map:  77%|███████▋  | 852/1100 [00:06<00:01, 130.16 examples/s]Map:  79%|███████▊  | 866/1100 [00:06<00:01, 131.07 examples/s]Map:  80%|████████  | 880/1100 [00:06<00:01, 132.40 examples/s]Map:  81%|████████▏ | 894/1100 [00:06<00:01, 133.51 examples/s]Map:  83%|████████▎ | 908/1100 [00:06<00:01, 133.74 examples/s]Map:  84%|████████▍ | 922/1100 [00:06<00:01, 132.12 examples/s]Map:  85%|████████▌ | 936/1100 [00:07<00:01, 132.47 examples/s]Map:  86%|████████▋ | 950/1100 [00:07<00:01, 133.09 examples/s]Map:  88%|████████▊ | 964/1100 [00:07<00:01, 133.42 examples/s]Map:  89%|████████▉ | 978/1100 [00:07<00:00, 133.41 examples/s]Map:  90%|█████████ | 992/1100 [00:07<00:00, 133.50 examples/s]Map:  92%|█████████▏| 1007/1100 [00:07<00:01, 72.21 examples/s]Map:  93%|█████████▎| 1021/1100 [00:07<00:00, 84.04 examples/s]Map:  94%|█████████▍| 1035/1100 [00:08<00:00, 94.77 examples/s]Map:  95%|█████████▌| 1050/1100 [00:08<00:00, 105.03 examples/s]Map:  97%|█████████▋| 1064/1100 [00:08<00:00, 111.79 examples/s]Map:  98%|█████████▊| 1078/1100 [00:08<00:00, 118.29 examples/s]Map:  99%|█████████▉| 1092/1100 [00:08<00:00, 123.31 examples/s]                                                                wandb: Currently logged in as: letshevatry. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /data/haotian/RAP_tune/alpaca-lora/wandb/run-20230723_062840-a9jybuxr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run July-22
wandb: ⭐️ View project at https://wandb.ai/letshevatry/30B-LLAMA-8bit
wandb: 🚀 View run at https://wandb.ai/letshevatry/30B-LLAMA-8bit/runs/a9jybuxr
  0%|          | 0/204 [00:00<?, ?it/s]/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/204 [02:47<9:27:26, 167.72s/it]  1%|          | 2/204 [05:35<9:24:28, 167.67s/it]  1%|▏         | 3/204 [08:24<9:23:57, 168.34s/it]  2%|▏         | 4/204 [11:08<9:16:04, 166.82s/it]  2%|▏         | 5/204 [13:57<9:15:03, 167.35s/it]  3%|▎         | 6/204 [16:40<9:07:22, 165.87s/it]  3%|▎         | 7/204 [19:26<9:04:43, 165.90s/it]  4%|▍         | 8/204 [22:08<8:58:21, 164.80s/it]  4%|▍         | 9/204 [24:56<8:58:54, 165.82s/it]  5%|▍         | 10/204 [27:45<8:58:37, 166.59s/it]                                                     5%|▍         | 10/204 [27:45<8:58:37, 166.59s/it]  5%|▌         | 11/204 [30:29<8:53:58, 166.00s/it]  6%|▌         | 12/204 [33:16<8:52:25, 166.38s/it]  6%|▋         | 13/204 [36:03<8:49:18, 166.27s/it]  7%|▋         | 14/204 [38:47<8:44:56, 165.77s/it]  7%|▋         | 15/204 [41:35<8:44:14, 166.42s/it]  8%|▊         | 16/204 [44:23<8:42:46, 166.84s/it]  8%|▊         | 17/204 [47:05<8:35:40, 165.46s/it]  9%|▉         | 18/204 [49:57<8:38:43, 167.33s/it]  9%|▉         | 19/204 [52:45<8:36:37, 167.56s/it] 10%|▉         | 20/204 [55:28<8:29:31, 166.15s/it]                                                    10%|▉         | 20/204 [55:28<8:29:31, 166.15s/it] 10%|█         | 21/204 [58:17<8:29:40, 167.11s/it] 11%|█         | 22/204 [1:01:04<8:26:37, 167.02s/it] 11%|█▏        | 23/204 [1:03:47<8:20:16, 165.84s/it] 12%|█▏        | 24/204 [1:06:34<8:18:32, 166.18s/it] 12%|█▏        | 25/204 [1:09:23<8:18:11, 166.99s/it] 13%|█▎        | 26/204 [1:12:07<8:12:55, 166.16s/it] 13%|█▎        | 27/204 [1:14:57<8:13:19, 167.23s/it] 14%|█▎        | 28/204 [1:17:43<8:09:51, 167.00s/it] 14%|█▍        | 29/204 [1:20:31<8:08:01, 167.32s/it] 15%|█▍        | 30/204 [1:23:17<8:03:49, 166.84s/it]                                                      15%|█▍        | 30/204 [1:23:17<8:03:49, 166.84s/it] 15%|█▌        | 31/204 [1:26:01<7:58:56, 166.11s/it] 16%|█▌        | 32/204 [1:28:50<7:58:36, 166.96s/it] 16%|█▌        | 33/204 [1:31:38<7:56:18, 167.13s/it] 17%|█▋        | 34/204 [1:34:27<7:55:13, 167.73s/it] 17%|█▋        | 35/204 [1:37:13<7:50:40, 167.10s/it] 18%|█▊        | 36/204 [1:39:58<7:46:47, 166.71s/it] 18%|█▊        | 37/204 [1:42:48<7:46:00, 167.43s/it] 19%|█▊        | 38/204 [1:45:31<7:40:10, 166.33s/it] 19%|█▉        | 39/204 [1:48:23<7:41:37, 167.86s/it] 20%|█▉        | 40/204 [1:51:09<7:37:36, 167.42s/it]                                                      20%|█▉        | 40/204 [1:51:09<7:37:36, 167.42s/it]/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 20%|██        | 41/204 [1:53:47<7:26:58, 164.53s/it] 21%|██        | 42/204 [1:56:33<7:25:10, 164.88s/it] 21%|██        | 43/204 [1:59:22<7:26:05, 166.25s/it] 22%|██▏       | 44/204 [2:02:11<7:25:44, 167.15s/it] 22%|██▏       | 45/204 [2:04:52<7:17:35, 165.13s/it] 23%|██▎       | 46/204 [2:07:36<7:14:12, 164.89s/it] 23%|██▎       | 47/204 [2:10:24<7:13:50, 165.80s/it] 24%|██▎       | 48/204 [2:13:09<7:10:49, 165.70s/it] 24%|██▍       | 49/204 [2:15:56<7:08:56, 166.04s/it] 25%|██▍       | 50/204 [2:18:48<7:10:19, 167.66s/it]                                                      25%|██▍       | 50/204 [2:18:48<7:10:19, 167.66s/it] 25%|██▌       | 51/204 [2:21:31<7:04:11, 166.35s/it] 25%|██▌       | 52/204 [2:24:17<7:01:26, 166.36s/it] 26%|██▌       | 53/204 [2:27:10<7:03:08, 168.13s/it] 26%|██▋       | 54/204 [2:30:01<7:02:31, 169.01s/it] 27%|██▋       | 55/204 [2:32:49<6:59:07, 168.78s/it] 27%|██▋       | 56/204 [2:35:32<6:52:08, 167.08s/it] 28%|██▊       | 57/204 [2:38:17<6:47:43, 166.42s/it] 28%|██▊       | 58/204 [2:40:59<6:42:03, 165.23s/it] 29%|██▉       | 59/204 [2:43:43<6:38:20, 164.83s/it] 29%|██▉       | 60/204 [2:46:31<6:37:30, 165.63s/it]                                                      29%|██▉       | 60/204 [2:46:31<6:37:30, 165.63s/it] 30%|██▉       | 61/204 [2:49:16<6:34:08, 165.38s/it] 30%|███       | 62/204 [2:52:03<6:32:31, 165.86s/it] 31%|███       | 63/204 [2:54:48<6:29:06, 165.58s/it] 31%|███▏      | 64/204 [2:57:28<6:23:07, 164.20s/it] 32%|███▏      | 65/204 [3:00:18<6:24:23, 165.92s/it] 32%|███▏      | 66/204 [3:03:04<6:21:42, 165.96s/it] 33%|███▎      | 67/204 [3:05:47<6:16:39, 164.96s/it] 33%|███▎      | 68/204 [3:08:34<6:15:07, 165.49s/it] 34%|███▍      | 69/204 [3:11:23<6:15:08, 166.73s/it] 34%|███▍      | 70/204 [3:14:07<6:10:21, 165.83s/it]                                                      34%|███▍      | 70/204 [3:14:07<6:10:21, 165.83s/it] 35%|███▍      | 71/204 [3:16:54<6:08:12, 166.11s/it] 35%|███▌      | 72/204 [3:19:39<6:04:55, 165.88s/it] 36%|███▌      | 73/204 [3:22:23<6:00:39, 165.18s/it] 36%|███▋      | 74/204 [3:25:10<5:58:51, 165.63s/it] 37%|███▋      | 75/204 [3:27:57<5:57:04, 166.08s/it] 37%|███▋      | 76/204 [3:30:44<5:55:07, 166.46s/it] 38%|███▊      | 77/204 [3:33:26<5:49:11, 164.97s/it] 38%|███▊      | 78/204 [3:36:11<5:46:51, 165.17s/it] 39%|███▊      | 79/204 [3:38:56<5:44:08, 165.18s/it] 39%|███▉      | 80/204 [3:41:42<5:41:37, 165.30s/it]                                                      39%|███▉      | 80/204 [3:41:42<5:41:37, 165.30s/it]/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 40%|███▉      | 81/204 [3:44:31<5:41:21, 166.51s/it] 40%|████      | 82/204 [3:47:16<5:37:17, 165.88s/it] 41%|████      | 83/204 [3:50:03<5:35:41, 166.46s/it] 41%|████      | 84/204 [3:52:56<5:36:42, 168.35s/it] 42%|████▏     | 85/204 [3:55:41<5:32:00, 167.40s/it] 42%|████▏     | 86/204 [3:58:33<5:31:27, 168.54s/it] 43%|████▎     | 87/204 [4:01:24<5:30:32, 169.51s/it] 43%|████▎     | 88/204 [4:04:14<5:27:43, 169.51s/it] 44%|████▎     | 89/204 [4:07:02<5:23:58, 169.03s/it] 44%|████▍     | 90/204 [4:09:50<5:20:40, 168.78s/it]                                                      44%|████▍     | 90/204 [4:09:50<5:20:40, 168.78s/it] 45%|████▍     | 91/204 [4:12:35<5:15:32, 167.55s/it] 45%|████▌     | 92/204 [4:15:23<5:13:14, 167.81s/it] 46%|████▌     | 93/204 [4:18:04<5:06:47, 165.83s/it] 46%|████▌     | 94/204 [4:20:50<5:03:41, 165.65s/it] 47%|████▋     | 95/204 [4:23:36<5:01:09, 165.77s/it] 47%|████▋     | 96/204 [4:26:23<4:59:06, 166.17s/it] 48%|████▊     | 97/204 [4:29:12<4:58:02, 167.13s/it] 48%|████▊     | 98/204 [4:32:00<4:55:35, 167.31s/it] 49%|████▊     | 99/204 [4:34:45<4:51:25, 166.53s/it] 49%|████▉     | 100/204 [4:37:34<4:49:58, 167.29s/it]                                                       49%|████▉     | 100/204 [4:37:34<4:49:58, 167.29s/it] 50%|████▉     | 101/204 [4:40:24<4:48:42, 168.18s/it] 50%|█████     | 102/204 [4:43:09<4:44:12, 167.19s/it] 50%|█████     | 103/204 [4:45:55<4:40:46, 166.80s/it] 51%|█████     | 104/204 [4:48:40<4:37:32, 166.53s/it] 51%|█████▏    | 105/204 [4:51:24<4:33:20, 165.66s/it] 52%|█████▏    | 106/204 [4:54:14<4:32:53, 167.07s/it] 52%|█████▏    | 107/204 [4:57:04<4:31:13, 167.77s/it] 53%|█████▎    | 108/204 [4:59:50<4:27:42, 167.32s/it] 53%|█████▎    | 109/204 [5:02:39<4:25:38, 167.78s/it] 54%|█████▍    | 110/204 [5:05:19<4:18:59, 165.32s/it]                                                       54%|█████▍    | 110/204 [5:05:19<4:18:59, 165.32s/it] 54%|█████▍    | 111/204 [5:08:03<4:15:39, 164.95s/it] 55%|█████▍    | 112/204 [5:10:55<4:16:16, 167.14s/it] 55%|█████▌    | 113/204 [5:13:39<4:11:57, 166.13s/it] 56%|█████▌    | 114/204 [5:16:21<4:07:27, 164.97s/it] 56%|█████▋    | 115/204 [5:19:07<4:05:02, 165.19s/it] 57%|█████▋    | 116/204 [5:21:46<3:59:49, 163.52s/it] 57%|█████▋    | 117/204 [5:24:34<3:59:06, 164.90s/it] 58%|█████▊    | 118/204 [5:27:24<3:58:26, 166.36s/it] 58%|█████▊    | 119/204 [5:30:13<3:56:33, 166.99s/it] 59%|█████▉    | 120/204 [5:33:00<3:53:58, 167.13s/it]                                                       59%|█████▉    | 120/204 [5:33:00<3:53:58, 167.13s/it]/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 59%|█████▉    | 121/204 [5:35:45<3:50:08, 166.37s/it] 60%|█████▉    | 122/204 [5:38:28<3:46:03, 165.41s/it] 60%|██████    | 123/204 [5:41:11<3:42:16, 164.65s/it] 61%|██████    | 124/204 [5:43:54<3:39:09, 164.37s/it] 61%|██████▏   | 125/204 [5:46:41<3:37:18, 165.04s/it] 62%|██████▏   | 126/204 [5:49:26<3:34:35, 165.07s/it] 62%|██████▏   | 127/204 [5:52:09<3:30:47, 164.26s/it] 63%|██████▎   | 128/204 [5:54:54<3:28:36, 164.69s/it] 63%|██████▎   | 129/204 [5:57:42<3:26:51, 165.49s/it] 64%|██████▎   | 130/204 [6:00:27<3:24:10, 165.54s/it]                                                       64%|██████▎   | 130/204 [6:00:27<3:24:10, 165.54s/it] 64%|██████▍   | 131/204 [6:03:09<3:19:59, 164.37s/it] 65%|██████▍   | 132/204 [6:05:56<3:18:18, 165.26s/it] 65%|██████▌   | 133/204 [6:08:47<3:17:25, 166.84s/it] 66%|██████▌   | 134/204 [6:11:33<3:14:33, 166.76s/it] 66%|██████▌   | 135/204 [6:14:16<3:10:17, 165.47s/it] 67%|██████▋   | 136/204 [6:17:01<3:07:32, 165.48s/it] 67%|██████▋   | 137/204 [6:19:47<3:04:50, 165.53s/it] 68%|██████▊   | 138/204 [6:22:31<3:01:42, 165.19s/it] 68%|██████▊   | 139/204 [6:25:16<2:58:37, 164.88s/it] 69%|██████▊   | 140/204 [6:28:06<2:57:38, 166.54s/it]                                                       69%|██████▊   | 140/204 [6:28:06<2:57:38, 166.54s/it] 69%|██████▉   | 141/204 [6:30:53<2:55:10, 166.84s/it] 70%|██████▉   | 142/204 [6:33:44<2:53:41, 168.09s/it] 70%|███████   | 143/204 [6:36:35<2:51:32, 168.72s/it] 71%|███████   | 144/204 [6:39:22<2:48:26, 168.44s/it] 71%|███████   | 145/204 [6:42:09<2:44:56, 167.74s/it] 72%|███████▏  | 146/204 [6:44:56<2:41:56, 167.53s/it] 72%|███████▏  | 147/204 [6:47:38<2:37:44, 166.05s/it] 73%|███████▎  | 148/204 [6:50:25<2:35:17, 166.38s/it] 73%|███████▎  | 149/204 [6:53:12<2:32:39, 166.54s/it] 74%|███████▎  | 150/204 [6:56:00<2:30:12, 166.89s/it]                                                       74%|███████▎  | 150/204 [6:56:00<2:30:12, 166.89s/it] 74%|███████▍  | 151/204 [6:58:46<2:27:10, 166.62s/it] 75%|███████▍  | 152/204 [7:01:34<2:24:48, 167.09s/it] 75%|███████▌  | 153/204 [7:04:24<2:22:48, 168.00s/it] 75%|███████▌  | 154/204 [7:07:07<2:18:39, 166.40s/it] 76%|███████▌  | 155/204 [7:09:52<2:15:31, 165.95s/it] 76%|███████▋  | 156/204 [7:12:42<2:13:42, 167.13s/it] 77%|███████▋  | 157/204 [7:15:26<2:10:19, 166.37s/it] 77%|███████▋  | 158/204 [7:18:16<2:08:21, 167.43s/it] 78%|███████▊  | 159/204 [7:20:59<2:04:34, 166.11s/it] 78%|███████▊  | 160/204 [7:23:46<2:01:56, 166.28s/it]                                                       78%|███████▊  | 160/204 [7:23:46<2:01:56, 166.28s/it]/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 79%|███████▉  | 161/204 [7:26:28<1:58:18, 165.07s/it] 79%|███████▉  | 162/204 [7:29:11<1:55:08, 164.49s/it] 80%|███████▉  | 163/204 [7:31:59<1:53:04, 165.47s/it] 80%|████████  | 164/204 [7:34:42<1:49:54, 164.86s/it] 81%|████████  | 165/204 [7:37:24<1:46:33, 163.92s/it] 81%|████████▏ | 166/204 [7:40:07<1:43:38, 163.66s/it] 82%|████████▏ | 167/204 [7:42:49<1:40:36, 163.14s/it] 82%|████████▏ | 168/204 [7:45:33<1:38:04, 163.46s/it] 83%|████████▎ | 169/204 [7:48:15<1:35:01, 162.91s/it] 83%|████████▎ | 170/204 [7:51:02<1:32:59, 164.09s/it]                                                       83%|████████▎ | 170/204 [7:51:02<1:32:59, 164.09s/it] 84%|████████▍ | 171/204 [7:53:49<1:30:44, 165.00s/it] 84%|████████▍ | 172/204 [7:56:38<1:28:35, 166.10s/it] 85%|████████▍ | 173/204 [7:59:18<1:24:55, 164.38s/it] 85%|████████▌ | 174/204 [8:02:10<1:23:20, 166.69s/it] 86%|████████▌ | 175/204 [8:05:02<1:21:20, 168.31s/it] 86%|████████▋ | 176/204 [8:07:45<1:17:47, 166.71s/it] 87%|████████▋ | 177/204 [8:10:37<1:15:43, 168.29s/it] 87%|████████▋ | 178/204 [8:13:21<1:12:21, 166.98s/it] 88%|████████▊ | 179/204 [8:16:11<1:09:54, 167.79s/it] 88%|████████▊ | 180/204 [8:18:59<1:07:11, 167.97s/it]                                                       88%|████████▊ | 180/204 [8:18:59<1:07:11, 167.97s/it] 89%|████████▊ | 181/204 [8:21:41<1:03:43, 166.22s/it] 89%|████████▉ | 182/204 [8:24:31<1:01:19, 167.26s/it] 90%|████████▉ | 183/204 [8:27:18<58:30, 167.16s/it]   90%|█████████ | 184/204 [8:30:01<55:18, 165.92s/it] 91%|█████████ | 185/204 [8:32:48<52:40, 166.32s/it] 91%|█████████ | 186/204 [8:35:31<49:36, 165.34s/it] 92%|█████████▏| 187/204 [8:38:18<46:55, 165.63s/it] 92%|█████████▏| 188/204 [8:41:09<44:37, 167.32s/it] 93%|█████████▎| 189/204 [8:43:53<41:34, 166.30s/it] 93%|█████████▎| 190/204 [8:46:38<38:44, 166.02s/it]                                                     93%|█████████▎| 190/204 [8:46:38<38:44, 166.02s/it] 94%|█████████▎| 191/204 [8:49:24<35:57, 165.95s/it] 94%|█████████▍| 192/204 [8:52:12<33:17, 166.45s/it] 95%|█████████▍| 193/204 [8:54:55<30:20, 165.52s/it] 95%|█████████▌| 194/204 [8:57:33<27:13, 163.34s/it] 96%|█████████▌| 195/204 [9:00:19<24:37, 164.19s/it] 96%|█████████▌| 196/204 [9:03:02<21:49, 163.65s/it] 97%|█████████▋| 197/204 [9:05:49<19:14, 164.88s/it] 97%|█████████▋| 198/204 [9:08:32<16:26, 164.34s/it] 98%|█████████▊| 199/204 [9:11:23<13:51, 166.22s/it] 98%|█████████▊| 200/204 [9:14:10<11:05, 166.36s/it]                                                     98%|█████████▊| 200/204 [9:14:10<11:05, 166.36s/it]/data/haotian/anaconda3/envs/RAP/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 99%|█████████▊| 201/204 [9:16:55<08:18, 166.10s/it] 99%|█████████▉| 202/204 [9:19:43<05:32, 166.46s/it]100%|█████████▉| 203/204 [9:22:28<02:46, 166.01s/it]100%|██████████| 204/204 [9:25:18<00:00, 167.24s/it]                                                    100%|██████████| 204/204 [9:25:18<00:00, 167.24s/it]100%|██████████| 204/204 [9:25:25<00:00, 166.30s/it]
{'loss': 0.1489, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.15}
{'loss': 0.144, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.29}
{'loss': 0.1278, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.44}
{'loss': 0.1157, 'learning_rate': 0.00011999999999999999, 'epoch': 0.58}
{'loss': 0.117, 'learning_rate': 0.00015, 'epoch': 0.73}
{'loss': 0.1077, 'learning_rate': 0.00017999999999999998, 'epoch': 0.87}
{'loss': 0.1083, 'learning_rate': 0.00020999999999999998, 'epoch': 1.02}
{'loss': 0.1066, 'learning_rate': 0.00023999999999999998, 'epoch': 1.16}
{'loss': 0.1045, 'learning_rate': 0.00027, 'epoch': 1.31}
{'loss': 0.1082, 'learning_rate': 0.0003, 'epoch': 1.45}
{'loss': 0.1032, 'learning_rate': 0.00027115384615384615, 'epoch': 1.6}
{'loss': 0.11, 'learning_rate': 0.0002423076923076923, 'epoch': 1.75}
{'loss': 0.1091, 'learning_rate': 0.00021346153846153845, 'epoch': 1.89}
{'loss': 0.1067, 'learning_rate': 0.0001846153846153846, 'epoch': 2.04}
{'loss': 0.1013, 'learning_rate': 0.00015576923076923076, 'epoch': 2.18}
{'loss': 0.0986, 'learning_rate': 0.0001269230769230769, 'epoch': 2.33}
{'loss': 0.0874, 'learning_rate': 9.807692307692307e-05, 'epoch': 2.47}
{'loss': 0.0875, 'learning_rate': 6.923076923076922e-05, 'epoch': 2.62}
{'loss': 0.091, 'learning_rate': 4.038461538461538e-05, 'epoch': 2.76}
{'loss': 0.0917, 'learning_rate': 1.1538461538461538e-05, 'epoch': 2.91}
{'train_runtime': 33921.9682, 'train_samples_per_second': 0.097, 'train_steps_per_second': 0.006, 'train_loss': 0.10828173525777518, 'epoch': 2.97}

 If there's a warning about missing keys above, please disregard :)
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:              train/global_step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇▇██
wandb:            train/learning_rate ▁▂▃▄▄▅▆▇▇█▇▇▆▅▄▄▃▂▂▁
wandb:                     train/loss █▇▆▄▄▃▃▃▃▃▃▄▃▃▃▂▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 2.97
wandb:              train/global_step 204
wandb:            train/learning_rate 1e-05
wandb:                     train/loss 0.0917
wandb:               train/total_flos 1.0101352569202852e+18
wandb:               train/train_loss 0.10828
wandb:            train/train_runtime 33921.9682
wandb: train/train_samples_per_second 0.097
wandb:   train/train_steps_per_second 0.006
wandb: 
wandb: 🚀 View run July-22 at: https://wandb.ai/letshevatry/30B-LLAMA-8bit/runs/a9jybuxr
wandb: Synced 6 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230723_062840-a9jybuxr/logs
